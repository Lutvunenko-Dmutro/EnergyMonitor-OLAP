import os
import logging
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations
from collections import Counter
from contextlib import contextmanager
import psycopg2
from dotenv import load_dotenv

# --- 1. CONFIGURATION ---
load_dotenv()

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–∏–ª—é –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

DB_CONFIG = {
    "dbname": os.getenv("DB_NAME", "postgres"),
    "user": os.getenv("DB_USER", "postgres"),
    "password": os.getenv("DB_PASSWORD", "password"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432")
}

# --- 2. DATABASE HELPER ---
@contextmanager
def get_db_connection():
    """–ë–µ–∑–ø–µ—á–Ω–µ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ë–î (—è–∫ —É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ñ)."""
    conn = None
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        yield conn
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ë–î: {e}")
        raise
    finally:
        if conn:
            conn.close()

def get_data(query: str) -> pd.DataFrame:
    """–í–∏–∫–æ–Ω—É—î SQL-–∑–∞–ø–∏—Ç —ñ –ø–æ–≤–µ—Ä—Ç–∞—î Pandas DataFrame."""
    with get_db_connection() as conn:
        return pd.read_sql_query(query, conn)

# --- 3. ANALYSIS FUNCTIONS (–î–µ–∫–æ–º–ø–æ–∑–∏—Ü—ñ—è) ---

def analyze_consumption_trends():
    """–ê–Ω–∞–ª—ñ–∑ —Ç—Ä–µ–Ω–¥—ñ–≤ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è (Innovators/Early Adopters)."""
    logger.info("üìà –ê–Ω–∞–ª—ñ–∑ —Ç—Ä–µ–Ω–¥—ñ–≤ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è...")

    sql = """
    SELECT 
        s.substation_name,
        EXTRACT(WEEK FROM l.timestamp) as week_num,
        AVG(l.actual_load_mw) as avg_load
    FROM LoadMeasurements l
    JOIN Substations s ON l.substation_id = s.substation_id
    GROUP BY s.substation_name, week_num
    ORDER BY s.substation_name, week_num;
    """
    df = get_data(sql)

    if df.empty:
        logger.warning("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç—Ä–µ–Ω–¥—ñ–≤.")
        return

    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plt.figure(figsize=(14, 7))
    sns.lineplot(
        data=df, 
        x='week_num', 
        y='avg_load', 
        hue='substation_name', 
        marker='o', 
        palette='tab10',
        linewidth=2.5
    )
    
    plt.title('–î–∏–Ω–∞–º—ñ–∫–∞ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ —Ç–∏–∂–Ω—è—Ö', fontsize=16)
    plt.xlabel('–ù–æ–º–µ—Ä —Ç–∏–∂–Ω—è', fontsize=12)
    plt.ylabel('–°–µ—Ä–µ–¥–Ω—î –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (–ú–í—Ç)', fontsize=12)
    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0.)
    plt.tight_layout()
    
    filename = 'trends_innovators.png'
    plt.savefig(filename)
    logger.info(f"‚úÖ –ì—Ä–∞—Ñ—ñ–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {filename}")

def analyze_cascading_failures():
    """–ü–æ—à—É–∫ –∞—Å–æ—Ü—ñ–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª —Ç–∞ –ø–æ–±—É–¥–æ–≤–∞ —Ç–µ–ø–ª–æ–≤–æ—ó –∫–∞—Ä—Ç–∏ –∫–æ—Ä–µ–ª—è—Ü—ñ–π."""
    logger.info("üîó –ê–Ω–∞–ª—ñ–∑ –∫–∞—Å–∫–∞–¥–Ω–∏—Ö –∞–≤–∞—Ä—ñ–π...")

    sql = """
    SELECT 
        date_trunc('hour', timestamp) as alert_time,
        s.substation_name
    FROM Alerts a
    JOIN Substations s ON a.substation_id = s.substation_id
    WHERE a.alert_type = '–ü–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è'
    GROUP BY alert_time, s.substation_name;
    """
    df = get_data(sql)

    if df.empty:
        logger.warning("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –ø—Ä–æ –∞–≤–∞—Ä—ñ—ó.")
        return

    # –ì—Ä—É–ø—É—î–º–æ: –ß–∞—Å -> –°–ø–∏—Å–æ–∫ –ø—ñ–¥—Å—Ç–∞–Ω—Ü—ñ–π
    baskets = df.groupby('alert_time')['substation_name'].apply(list).values

    # –†–∞—Ö—É—î–º–æ –ø–∞—Ä–∏
    pair_counts = Counter()
    all_substations = set()

    for basket in baskets:
        unique_subs = sorted(list(set(basket))) # –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –¥—É–±–ª—ñ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –æ–¥–Ω—ñ—î—ó –≥–æ–¥–∏–Ω–∏
        all_substations.update(unique_subs)
        if len(unique_subs) > 1:
            pair_counts.update(combinations(unique_subs, 2))

    # –í–∏–≤—ñ–¥ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–≤—ñ—Ç—É
    print("\n" + "="*50)
    print(" –¢–û–ü-5 –ü–ê–† –ü–Ü–î–°–¢–ê–ù–¶–Ü–ô (–°–ø—ñ–ª—å–Ω—ñ –∞–≤–∞—Ä—ñ—ó)")
    print("="*50)
    top_pairs = pair_counts.most_common(5)
    for (s1, s2), count in top_pairs:
        print(f"{s1:<25} + {s2:<25} | {count} —Ä–∞–∑—ñ–≤")

    # --- –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø: –¢–ï–ü–õ–û–í–ê –ö–ê–†–¢–ê (Heatmap) ---
    subs_list = sorted(list(all_substations))
    matrix = pd.DataFrame(0, index=subs_list, columns=subs_list)

    for (s1, s2), count in pair_counts.items():
        matrix.loc[s1, s2] = count
        matrix.loc[s2, s1] = count # –°–∏–º–µ—Ç—Ä–∏—á–Ω–æ

    if not matrix.empty and matrix.sum().sum() > 0:
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            matrix, 
            annot=True, 
            fmt="d", 
            cmap="Reds", 
            linewidths=.5,
            cbar_kws={'label': '–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–ª—å–Ω–∏—Ö —ñ–Ω—Ü–∏–¥–µ–Ω—Ç—ñ–≤'}
        )
        plt.title('–ú–∞—Ç—Ä–∏—Ü—è –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –∞–≤–∞—Ä—ñ–π (–•—Ç–æ –ø–∞–¥–∞—î —Ä–∞–∑–æ–º?)', fontsize=16)
        plt.tight_layout()
        
        filename = 'heatmap_failures.png'
        plt.savefig(filename)
        logger.info(f"‚úÖ –¢–µ–ø–ª–æ–≤—É –∫–∞—Ä—Ç—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {filename}")
    else:
        logger.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ–ø–ª–æ–≤–æ—ó –∫–∞—Ä—Ç–∏.")

# --- 4. MAIN ---
if __name__ == "__main__":
    try:
        analyze_consumption_trends()
        analyze_cascading_failures()
        logger.info("üèÅ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
